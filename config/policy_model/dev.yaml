shared:
  output_path: ${USER}/policy_model/dev/1
  n_seq_timesteps: 200
  frame_height: 192
  frame_width: 192
  text_tokenizer_config:
    text_tokenizer_name: "gemma"
    text_embedding_shape: [1, 768]
    text_annotation_model_version: ["gemini-2.5-flash", "gemini-2.5-flash-thinking-0905"]
  tokenizer:
    type: "conv"
    conv_tokenizer_config:
      num_tokens: 1
    vit_tokenizer_config:
      patch_size: 16

inference:
  mouse_sampling_approach: "truncated_normal"
  checkpoint_path: iraki/policy_model/dev/1/stage3_finetune/checkpoint-step=00000500.ckpt

# Policy Model Configuration (Stage 2 student & 3 model)
policy_model:
  n_transformer_layers: 10
  n_q_head: 16
  n_kv_head: 16
  n_thinking_tokens: 1
  transformer_dim: &transformer_dim 1024
  mask_block_size: 128
  attention_history_len: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200] # the length of the attention history needs to be the same as n_transformer_layers
  n_kv_sink_tokens: 0
  model_type: "dense"
  sparse_moe:
    num_experts: 16
    experts_per_token: 4
  action_decoder:
    embed_dim: *transformer_dim


stage3_finetune:
  optim:
    learning_rate: 0.0001
    weight_decay: 0.0001
    beta_1: 0.9
    beta_2: 0.999

  validation_step_interval: 2_000_000 # super large number so it won't be triggered
  n_validation_steps: &n_validation_steps 200
  save_every_n_steps: 500
  n_training_steps: 200_000
  accumulate_grad_batches: 1

  training_dataset:
    batch_size: &stage3_batch_size 1
    local_prefix: data_folder/employee_labelled
    always_labelled: true
    n_preprocess_threads_per_gpu: 32
    preprocessed_chunks_queue_size_per_gpu: 2
    warn_on_starvation: false
    rand_augmentation:
      fraction_augmented: 1.0
      augmentations:
        - spatial_transform
        - color
        - planckian
        - iso_noise
        - translation
        - random_blur
        - sharpness
    shuffle_buffer_size_per_gpu: 1900
    dataset_worker_num_workers_per_gpu: 1
    dataset_worker_prefetch_factor: 16
    shuffled_chunks_queue_size_per_gpu: 16

  validation_datasets:
    - batch_size: *stage3_batch_size
      local_prefix: data_folder_validation/employee_labelled
      validation_name: "overall"
      always_labelled: true
      n_preprocess_threads_per_gpu: &validation_stage3_n_preprocess_threads_per_gpu 8
      preprocessed_chunks_queue_size_per_gpu: &validation_stage3_preprocessed_chunks_queue_size_per_gpu 1
      shuffled_chunks_queue_size_per_gpu: &validation_stage3_shuffled_chunks_queue_size 1
      dataset_worker_prefetch_factor: &validation_stage3_dataset_worker_prefetch_factor 8
      warn_on_starvation: false


wandb:
  enabled: true
  project: "policy_model"
  # Generally want adhoc experiments to show as ready for deletion.
  exp_name: "delete-me-dev"
